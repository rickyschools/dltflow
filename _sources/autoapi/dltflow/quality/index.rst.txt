:py:mod:`dltflow.quality`
=========================

.. py:module:: dltflow.quality


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   config/index.rst
   dlt_meta/index.rst
   exceptions/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   dltflow.quality.DLTMetaMixin




.. py:class:: DLTMetaMixin


   A metaclass that wraps a function with DLT expectations. This metaclass is designed
   to be used in conjunction with the DLTConfig class. The DLTConfig class is a pydantic model
   that is used to configure the DLTMetaMixin class. The DLTMetaMixin class is a metaclass that
   wraps a function with DLT expectations.

   The intent of this metaclass is to:

   1. To provide an alternative way of writing DLT pipelines that is framework-driven and not
       only intended for notebook development.
   2. A meta-programming (config driven) approach to authoring DLT pipelines.

   .. py:method:: _set_write_opts_if_needed(config)

      Set the write options if the config has a writer attribute and the dlt config doesn't have write_opts.

      :param config: The configuration for the class.
      :type config: dict


   .. py:method:: _set_child_func_attributes()

      Set the attributes for the child functions based on the execution configuration.


   .. py:method:: _make_execution_plan(configs: dltflow.quality.config.DLTConfigs)

      Create an execution plan for the user function.

      :param configs: The configuration for the execution plan.
      :type configs: DLTConfigs

      :returns: The execution plan.
      :rtype: list


   .. py:method:: _get_dlt_config(config) -> dltflow.quality.config.DLTConfigs
      :staticmethod:

      A static method to get the DLTConfig object from the provided configuration.

      :param config:

      :returns: The DLTConfig object that was created from the provided configuration.
      :rtype: DLTConfig


   .. py:method:: _dangerous_code_check(code: str)

      This method checks for dangerous code (defined according to Databricks' DLT guidelines) in
      the user-provided code/function.
      :param code:


   .. py:method:: _unsupported_action_check(code: str)

      This method checks for unsupported code (defined according to Databricks' DLT guidelines) in
      the user-provided code/function.
      :param code:


   .. py:method:: _return_type_check(signature: Union[inspect.Signature, Callable])

      Check if the return type of the user function is a Spark DataFrame.

      This method inspects the code and typed return annotations to ensure that the return type
      of the user function is a Spark DataFrame. If no return annotation is provided, a warning
      is issued. If the return type is not a Spark DataFrame, a TypeError is raised.

      :param signature: The signature of the user function.
      :type signature: inspect.Signature

      :raises TypeError: If the return type of the user function is not a Spark DataFrame.


   .. py:method:: _enforce_delta_limitations_and_requirements(func_name: str) -> Callable

      Checks for dangerous and unsupported code in the user function.

      :param func_name: The name of the user function.
      :type func_name: str

      :returns: The user function.
      :rtype: Callable


   .. py:method:: table_view_expectation_wrapper(child_function, execution_config)
      :staticmethod:

      This method is the "magic" that dynamically and automatically wraps the user function with DLT expectations.

      This is used for non-streaming tables and intermediate views in delta live table pipelines. If the user
      chooses, they can also apply expectations to their dataset.

      Read more about streaming tables in Databricks' documentation:
      https://docs.databricks.com/en/delta-live-tables/python-ref.html#example-define-tables-and-views

      :param child_function: The child function that will be wrapped with DLT expectations.
      :type child_function: callable
      :param execution_config: The configuration that will be used to execute the DLT expectations.
      :type execution_config: DLTExecutionConfig


   .. py:method:: streaming_table_expectation_wrapper(child_function, execution_config)
      :staticmethod:

      This method is a magic wrapper that applies streaming dlt operations to user queries/functions.

      This is used for streaming tables in delta live table pipelines. If the user chooses, they can also apply
      expectations to their dataset.

      Read more about streaming tables in Databricks' documentation:
      https://docs.databricks.com/en/delta-live-tables/python-ref.html

      Append Flow:
      https://docs.databricks.com/en/delta-live-tables/python-ref.html#write-to-a-streaming-table-from-multiple-source-streams

      Apply Changes:
      https://docs.databricks.com/en/delta-live-tables/python-ref.html#write-to-a-streaming-table-from-multiple-source-streams
      https://docs.databricks.com/en/delta-live-tables/cdc.html

      :param child_function: The child function that will be wrapped with DLT expectations.
      :type child_function: callable
      :param execution_config: The configuration that will be used to execute the DLT expectations.
      :type execution_config: DLTExecutionConfig


   .. py:method:: apply_dlt(child_function, execution_config: dltflow.quality.config.DLTExecutionConfig)

      This method is the "magic" that dynamically and automatically wraps the user function with DLT expectations.

      The method is a decorator that wraps the user function (specified in configuration) with the DLT
      expectations that were provided in the configuration. The return value of the user function is
      expected to be a Spark DataFrame. If the return value is not a Spark DataFrame,
      a TypeError will be raised.

      :param child_function: The child function that will be wrapped with DLT expectations.
      :type child_function: t.Callable
      :param execution_config: The configuration that will be used to execute the DLT expectations.
      :type execution_config: DLTExecutionConfig



